{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79da246d",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13fbce5",
   "metadata": {},
   "source": [
    "## Ensemble Theory\n",
    "\n",
    "Ensemble methods are a powerful family of approaches in machine-learning. The idea is to combine multiple individual models together, an ensemble, such that they collectively produce better predictions with greater generalisability. The individual models are often reffered to as base models or \"weak learners\" and can be constructed using a single modelling algorithm, or several different algorithms. Variability between the base models is desired such that their individual errors are less likely to overlap, allowing the ensemble to correct for mistakes made by any single model. This diversity helps reduce overfitting, improves robustness, and leads to more stable and accurate predictions overall. A common approach to creating an ensemble is **bagging** (bootstrap aggregating) which uses a technique known as **bootstrapping**. In bootstrapping, a given base model is trained on a random sample of the full training dataset. This is performed in parallel with each model indepedent from oneanother. In this section, we will implement a **random forest classifier** which is probably the most well-known ensemble model. In a generic random forest, the base model is decision tree and the final class prediction is determined from the majorite vote of the trees. Our implementation of a decision tree, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a54e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports,\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DecisionTreeClassifier():\n",
    "\n",
    "    def __init__(self, max_depth):\n",
    "        \"\"\"Constructor method for the DecisionTreeClassifier class. We simply create the class variables.\"\"\"\n",
    "        \n",
    "        # Class variables for the data and nodes,\n",
    "        self.X, self.y, self.nodes, self.leaves = None, None, None, None\n",
    "\n",
    "        # Stopping criteria,\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "\n",
    "        # Creating the root node,\n",
    "        root_node = Node(self.X, self.y, parent_node=None)\n",
    "        self.nodes, self.leaves = [root_node], [root_node]\n",
    "\n",
    "        # Growth algorithm,\n",
    "        for i in range(self.max_depth):\n",
    "            new_leaves = self.grow_tree()\n",
    "\n",
    "            # Adding child nodes,\n",
    "            self.nodes.extend(new_leaves)\n",
    "\n",
    "        # Assigning class predictions to leaves (they become terminate nodes) based on majority vote,\n",
    "        for leaf_node in self.leaves:\n",
    "            leaf_node.class_prediction = majority_vote(leaf_node.y)\n",
    "\n",
    "    def grow_tree(self):\n",
    "    \n",
    "        # Placeholder list,\n",
    "        new_leaves = []\n",
    "\n",
    "        # Looping through current leaves,\n",
    "        for node in self.leaves:\n",
    "\n",
    "            # Performing split,\n",
    "            child_node_left, child_node_right, valid_split = self.split(node.X, node.y, parent_node=node)\n",
    "\n",
    "            if valid_split:\n",
    "                # Assigning child nodes,\n",
    "                node.child_left, node.child_right, = child_node_left, child_node_right\n",
    "\n",
    "                # Appending nodes to list,\n",
    "                new_leaves.extend([child_node_left, child_node_right])\n",
    "            else:\n",
    "                # Node becomes a terminal node and is assigned a class prediction,\n",
    "                node.class_prediction = majority_vote(node.y)\n",
    "\n",
    "        # Updating leaves,\n",
    "        self.leaves = new_leaves\n",
    "\n",
    "        return new_leaves\n",
    "\n",
    "    def split(self, X, y, parent_node):\n",
    "        \"\"\"Binary splits a parent node into two child nodes based on the decision that maximises information gain in accordance\n",
    "        with Shannon entropy.\"\"\"\n",
    "\n",
    "        # Placeholder variables,\n",
    "        max_gain = -1\n",
    "        split_threshold_value = None\n",
    "        found_valid_split = False\n",
    "        X_best_left_split, X_best_right_split, y_best_left_split, y_best_right_split = None, None, None, None\n",
    "\n",
    "        # Computing entropy before split,\n",
    "        S_parent = compute_entropy(y)\n",
    "\n",
    "        # Randomly selecting features,\n",
    "        subset_size = np.random.randint(low=1, high=(X.shape[1]+1), size=1)[0]\n",
    "        feature_idxs = np.arange(start=0, stop=X.shape[1], step=1)\n",
    "        selected_feature_idxs = np.random.choice(feature_idxs, size=subset_size, replace=False)\n",
    "\n",
    "        # Double loop, first for each feature, second for each threshold value,\n",
    "        for feature_idx in selected_feature_idxs:\n",
    "\n",
    "            # Extracting feature values and thresholds,\n",
    "            X_feature = X[:, feature_idx]\n",
    "            thresholds = np.unique(X_feature)\n",
    "\n",
    "            for threshold_value in thresholds:\n",
    "\n",
    "                # Splitting data in parent node into child nodes,\n",
    "                left_split_idxs, right_split_idxs = np.where(X_feature <= threshold_value)[0], np.where(X_feature > threshold_value)[0]\n",
    "                X_left_split, X_right_split = X[left_split_idxs], X[right_split_idxs]\n",
    "                y_left_split, y_right_split = y[left_split_idxs], y[right_split_idxs]\n",
    "\n",
    "                # Reject splits which result in empty child nodes,\n",
    "                if len(left_split_idxs) == 0 or len(right_split_idxs) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    found_valid_split = True\n",
    "\n",
    "                # Compute entropy after split,\n",
    "                S_left, S_right = compute_entropy(y_left_split), compute_entropy(y_right_split)\n",
    "\n",
    "                # Calculating information gain,\n",
    "                w1, w2 = len(y_left_split)/len(y), len(y_right_split)/len(y)\n",
    "                delta_S = S_parent - (w1*S_left + w2*S_right)\n",
    "            \n",
    "                # Tracking maximum information gain,\n",
    "                if delta_S > max_gain:\n",
    "\n",
    "                    # Updating nodes associated with the best split,\n",
    "                    max_gain, split_threshold_value, split_feature = delta_S, threshold_value, feature_idx\n",
    "                    X_best_left_split, X_best_right_split, y_best_left_split, y_best_right_split = X_left_split, X_right_split, y_left_split, y_right_split\n",
    "\n",
    "        # Creating node objects for the child nodes,\n",
    "        if found_valid_split:\n",
    "            child_node_left, child_node_right = Node(X_best_left_split, y_best_left_split, parent_node), Node(X_best_right_split, y_best_right_split, parent_node)\n",
    "            parent_node.child_left, parent_node.child_right = child_node_left, child_node_right\n",
    "            parent_node.decision = (split_feature, split_threshold_value)\n",
    "            return child_node_left, child_node_right, True\n",
    "        else:\n",
    "            return None, None, False\n",
    "        \n",
    "    def predict_sample(self, X_sample):\n",
    "        current_node = self.nodes[0]\n",
    "\n",
    "        while current_node.decision is not None:\n",
    "            feature_idx, threshold_value = current_node.decision\n",
    "\n",
    "            if X_sample[feature_idx] <= threshold_value:\n",
    "                current_node = current_node.child_left\n",
    "            else:\n",
    "                current_node = current_node.child_right\n",
    "\n",
    "        return current_node.class_prediction\n",
    "\n",
    "    def score(self, X, y):\n",
    "\n",
    "        correct = 0\n",
    "        n_samples = X.shape[0]\n",
    "        for i in range(n_samples):\n",
    "            pred, target = self.predict_sample(X_sample=X[i]), y[i]\n",
    "\n",
    "            if pred == target:\n",
    "                correct += 1\n",
    "\n",
    "        accuracy = correct/n_samples\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def info(self):\n",
    "        for node in self.nodes:\n",
    "            node.info(verbose=True)\n",
    "\n",
    "class Node():\n",
    "    \"\"\"The class for node objects. Essentially used as a container.\"\"\"\n",
    "\n",
    "    def __init__(self, X, y, parent_node):\n",
    "        \"\"\"Constructor method for the node. Class variables contain node information and encode its location in the tree\n",
    "        required for predictions.\"\"\"\n",
    "\n",
    "        # Node information,\n",
    "        self.X, self.y = X, y\n",
    "        self.decision = None\n",
    "        self.class_prediction = None\n",
    "\n",
    "        # Encodes location in the tree,\n",
    "        self.parent, self.child_left, self.child_right = parent_node, None, None\n",
    "\n",
    "    def info(self, verbose=False):\n",
    "        \"\"\"Returns information about the node.\"\"\"\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Parent: {self.parent}, Decision: {self.decision}, Class prediction: {self.class_prediction}\")\n",
    "\n",
    "        return self.parent, self.decision, self.class_prediction\n",
    "\n",
    "def compute_entropy(y):\n",
    "    \"\"\"Helper function which computes the Shannon entropy of a given node.\"\"\"\n",
    "\n",
    "    # Computing probabilities P_j,\n",
    "    classes, classes_counts = np.unique(y, return_counts=True)\n",
    "    classes_probs = classes_counts/len(y)\n",
    "\n",
    "    # Computing the Shannon entropy of the node,\n",
    "    entropy = -np.sum(classes_probs*np.log2(classes_probs))\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def majority_vote(array):\n",
    "    \"\"\"Returns the most frequent class index.\"\"\"\n",
    "    return np.bincount(array).argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ba477",
   "metadata": {},
   "source": [
    "## Basic Implementation\n",
    "\n",
    "Typically, a random forest incorporates two design considerations which allows for variability between the trees. We have, \n",
    "\n",
    "<ul>\n",
    "  <li>Bootstrapping: As previously mentioned, each tree will be trained on a random subset of the full training dataset. This introduces variance in the predictions because there is now an element of stochasticity in our model. </li>\n",
    "  <li>Random Feature Selection: When splitting a node in a tree, we randomly select a subset of features to make the decision on. This is different from the way we previously coded our decision trees. In the \n",
    "  original code, the construction of a tree was purely deterministic because we considered all features whenever we made a split. The purpose of random feature selection is to discourage correlation between trees. That is, to \n",
    "  make it less likely that trees will have the same node structure from considering the same features. This means that the trees will be less likely to make the same mistakes. </li>\n",
    "</ul>\n",
    "\n",
    "Below, we have a basic implementation of a random forest classifier,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ab11a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier():\n",
    "    \"\"\"Class for the random forest ensemble model.\n",
    "\n",
    "    PARAMETERS\n",
    "    n_trees (int): The number of decision trees in the random forest.\n",
    "    max_depth (int): The maximum number of layers a given tree may have.\n",
    "    min_depth (int): The minimum number of layers a given tree may have.\n",
    "    bootstrap_ratio (0 < r < 1): This is the ratio between the number of samples in a given random subset over the full dataset.\n",
    "    randomise_depth (bool): Toggle whether or not to randomise the max depth of a given tree between min_depth and max_depth.\"\"\"\n",
    "\n",
    "    def __init__(self, n_trees, max_depth, min_depth=1, bootstrap_ratio=1, randomise_depth=False):\n",
    "        \"\"\"Constructor method for the random forest ensemble model. Placeholder and class variables are assigned using this method.\"\"\"\n",
    "\n",
    "        # Class variables,\n",
    "        self.n_trees, self.max_depth, self.min_depth = n_trees, max_depth, min_depth\n",
    "        self.bootstrap_ratio, self.randomise_depth = bootstrap_ratio, randomise_depth\n",
    "\n",
    "        # Ensemble list,\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"This method populates the random forest using the data supplied.\"\"\"\n",
    "\n",
    "        # A new tree is created in each iteration,\n",
    "        for i in range(self.n_trees):\n",
    "\n",
    "            # Selecting a subset of the training data (bootstrapping)\n",
    "            X_sub, y_sub = self.bootstrap(X, y)\n",
    "\n",
    "            # Determining the max depth of the tree,\n",
    "            if self.randomise_depth:\n",
    "                tree_depth = np.random.randint(self.min_depth, self.max_depth, 1)[0]\n",
    "            else:\n",
    "                tree_depth=self.max_depth\n",
    "            \n",
    "            # Creating and fitting tree,\n",
    "            tree = DecisionTreeClassifier(max_depth=tree_depth)\n",
    "            tree.fit(X_sub, y_sub)\n",
    "\n",
    "            # Adding the tree to the forest,\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict_sample(self, X_sample):\n",
    "        \"\"\"This function returns the class prediction of a single data sample.\"\"\"\n",
    "\n",
    "        # Placeholder,\n",
    "        trees_pred = []\n",
    "\n",
    "        # Looping over all trees,\n",
    "        for tree in self.trees:\n",
    "\n",
    "            # Individual tree prediction,\n",
    "            tree_pred = tree.predict_sample(X_sample)\n",
    "            trees_pred.append(tree_pred)\n",
    "\n",
    "        # Finding majority-vote,\n",
    "        trees_pred = np.array(trees_pred)\n",
    "        model_pred = majority_vote(trees_pred)\n",
    "\n",
    "        # Returning model prediction,\n",
    "        return model_pred\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"Returns the prediction accuracy of the model on a given dataset.\"\"\"\n",
    "\n",
    "        # Storing predictions as an array,\n",
    "        preds = np.array([self.predict_sample(x) for x in X])\n",
    "\n",
    "        # Calculating accuracy,\n",
    "        correct, n_samples = len(np.where(preds == y)[0]), len(y)\n",
    "        accuracy = correct/n_samples\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def bootstrap(self, X, y):\n",
    "        \"\"\"Helper function used to created a random subset from a larger set of data points. Returns a tuple.\"\"\"\n",
    "\n",
    "        # Determining the number of samples in the full set and subset,\n",
    "        n_samples = X.shape[0]\n",
    "        n_subsamples = int(self.bootstrap_ratio*n_samples)\n",
    "\n",
    "        # Computing the indices of the subset,\n",
    "        subset_idxs = np.random.choice(n_samples, size=n_subsamples, replace=True)\n",
    "\n",
    "        # Returning subset,\n",
    "        return X[subset_idxs], y[subset_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b72c4f0",
   "metadata": {},
   "source": [
    "Note that we also have the option to randomise the maximum depth of the trees in the forest. This is slightly atypical, but also discourages correlation between trees. We test our model on the Wine dataset,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe49adfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing,\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "# Loading dataset,\n",
    "wine_dataset = datasets.load_wine()\n",
    "\n",
    "# Extracting features,\n",
    "X, y = wine_dataset[\"data\"], wine_dataset[\"target\"]\n",
    "\n",
    "# Creating training split,\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=31)\n",
    "\n",
    "# Creating and fitting model,\n",
    "clf = RandomForestClassifier(n_trees=100, max_depth=5, randomise_depth=True)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
