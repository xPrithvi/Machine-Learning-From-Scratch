# Machine Learning From Scratch
The most common machine learning models and algorithms programmed from first principles. The aim is to implement these models and algorithms to the same standard as widely used libraries such as scikit-learn with clean, efficient code, clear modular design, and rigorous adherence to the underlying mathematics.

| Model / Algorithm            | Type                      | Completed? | Notes |
|------------------------------|---------------------------|----------|------------|
| Linear Regression            | Regression                | âœ…        | TBR        |
| Logistic Regression          | Classification            | âœ…        | TBR        |
| K-Nearest Neighbours         | Classification            | âœ…        | TBR        |
| K-Means                      | Clustering                | âœ…        | TBR        |
| Stochastic Gradient Descent  | Optimisation              | âœ…        | TBR        |
| Naive Bayes Classifiers       | Classification            | Categorical âœ…, Multinomial âŒ, Gaussian âŒ | TBR        |
| Decision Trees                | Classification/Regression | Classifier âœ…, Regressor âœ…, CCP Pruning âœ…, Feature Importance âœ… | Speed and efficiently need to be further optimised for production-level. |
| Random Forests                | Classification/Regression | Bootstrap aggregated âœ…, Rotation forest âŒ, Extremely Randomised Trees (ERT) âŒ | TBR        |
| Support Vector Machine       | Classification/Regression | â³ğŸš§ Hard-margin SVC âœ…, Soft-margin SVC âŒ Kernel Trick âŒ OvO âŒ OvR âŒ | CURRENTLY WORKING ON |
| Principal Component Analysis | Dimensionality Reduction  | âœ…        | First principle derivation of the eigenvalue equation need to be added. |
| DBSCAN                       | Clustering                | âœ…        | Pseudo-code needs to be added for queuing algorithm used for cluster growth. |
| Gaussian Mixture Models      | Clustering (Probabilistic)| â³ğŸš§      | In progress (put on hold). Need to formally explore Maximum Likelihood Estimation (MLE) theory. |
| Linear Discriminant Analysis | Dimensionality Reduction  | âŒ        | Not started.    |
| Gradient Boosting            | Classification/Regression | â³ğŸš§      | Theory section written, but the model has not been implemented. |

Current working on: Support Vector Machines
- [ ] Implement the kernel trick for SVCs.
- [ ] Implement a soft-margin SVC using the dual formulation.
- [ ] Implement a soft-margin SVC using SGD and Hinge loss.
- [ ] Combine multiple SVCs (OvO and OvR) to handle classification with C>2 number of classes.
